import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer

# --- 1. Define Genre Rules and Schema ---

HIPHOP_RULES = """
- 85-95 BPM (Use 90 BPM)
- 4/4 time signature
- Rhythm: Kick drum on beats 1 and 3; Snare on beats 2 and 4.
- Harmony: Minor keys. Use a simple 2-chord loop (e.g., C minor).
- Instruments: 808 drum machine (Channel 9), Synth Pad (Channel 0).
- Structure: Generate a 4-bar loop.
"""

TARGET_JSON_SCHEMA = """
{
  "tempo": [INTEGER BPM],
  "time_signature": "4/4",
  "length_bars": 4,
  "tracks": [
    {
      "name": "synth_pad_loop",
      "channel": 0,
      "notes": [
        // Note structure: { "bar": 1, "beat": 1.0, "pitch": 60, "duration": 4.0, "velocity": 85 }
      ]
    },
    {
      "name": "808_drum_machine",
      "channel": 9,
      "notes": [
        // Note structure: { "bar": 1, "beat": 1.0, "pitch": 36, "duration": 0.25, "velocity": 110 }
      ]
    }
  ]
}
"""

# --- 2. Load Model (Same as before) ---
model_path = "ibm-granite/granite-4.0-h-350m"
device = "cuda" if torch.cuda.is_available() else "cpu"
if torch.backends.mps.is_available(): device = "mps"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", torch_dtype=torch.bfloat16)
model.eval()

# --- 3. Construct the Structured Prompt ---
chat = [
    {"role": "system",
     "content": "You are a Music Generation Assistant. Your task is to generate a complete musical composition strictly in the JSON format provided. Do not include any explanatory text, commentary, or wrapper code blocks, only the JSON object itself."},
    {"role": "user", "content": f"""
    Please generate a 4-bar music loop following these musical rules:

    # Genre Rules:
    {HIPHOP_RULES}

    # Target JSON Format (Schema):
    {TARGET_JSON_SCHEMA}

    Generate the complete, fully populated JSON file based on the rules and the schema.
    """},
]

# --- 4. Generate Output ---
input_tokens = tokenizer.apply_chat_template(chat, return_tensors="pt", add_generation_prompt=True).to(device)

with torch.no_grad():
    output = model.generate(
        input_tokens,
        max_new_tokens=1500,  # Increased tokens to allow for full JSON generation
        do_sample=False,
        temperature=0.0
    )

decoded_output = tokenizer.decode(output[0], skip_special_tokens=False)

# --- 5. Extract and Validate JSON ---
# The model output is often wrapped in the chat template, so we must find the JSON part.
try:
    # This attempts to find the content generated by the 'assistant'
    json_start = decoded_output.find("<|start_of_role|>assistant<|end_of_role|>") + len(
        "<|start_of_role|>assistant<|end_of_role|>")
    json_end = decoded_output.find("<|end_of_text|>", json_start)

    raw_json = decoded_output[json_start:json_end].strip()

    # Clean up common LLM formatting issues (e.g., Markdown code blocks)
    if raw_json.startswith("```json"):
        raw_json = raw_json[7:]
    if raw_json.endswith("```"):
        raw_json = raw_json[:-3]

    # Final check and parsing
    generated_music_data = json.loads(raw_json)

    print("\n--- SUCCESSFULLY PARSED MUSIC JSON ---")
    print(json.dumps(generated_music_data, indent=2))

except Exception as e:
    print(f"\n--- ERROR PARSING JSON ---")
    print("Raw Model Output:")
    print(decoded_output)
    print(f"\nError: {e}")